# üêç src/

**Code source Python du projet test-gliner2**

Ce dossier contient tous les modules Python pour preprocessing, NER, et analyse de r√©seau.

---

## Structure

```
src/
‚îú‚îÄ‚îÄ preprocessing/    # Nettoyage et normalisation de texte
‚îú‚îÄ‚îÄ ner/              # Extraction NER avec GLiNER2
‚îú‚îÄ‚îÄ network/          # Construction et analyse de graphes
‚îî‚îÄ‚îÄ utils/            # Fonctions utilitaires communes
```

---

## üìÅ Modules

### `preprocessing/`
**R√¥le** : Nettoyer et normaliser le corpus brut Esperanto

**Scripts principaux** :
- `clean_text.py` - Nettoyage texte (Unicode, caract√®res sp√©ciaux)
- `normalize_esperanto.py` - Normalisation sp√©cifique Esperanto (ƒâ, ƒù, etc.)
- `segment_documents.py` - Segmentation en unit√©s logiques

**Exemple d'utilisation** :
```bash
python src/preprocessing/clean_text.py \
  --input data/raw/ \
  --output data/processed/ \
  --log data/processed/processing_log.json
```

**Tests** : `tests/test_preprocessing/`

---

### `ner/`
**R√¥le** : Extraction d'entit√©s nomm√©es avec GLiNER2

**Scripts principaux** :
- `gliner_extractor.py` - Extraction NER zeroshot avec GLiNER2
- `validate_ner.py` - Validation manuelle et calcul de m√©triques (F1, pr√©cision, rappel)
- `post_process.py` - Post-traitement (d√©duplication, normalisation variantes)

**Exemple d'utilisation** :
```bash
# Extraction NER
python src/ner/gliner_extractor.py \
  --input data/processed/ \
  --output data/annotated/ \
  --model models/checkpoints/gliner_multi-v2.1 \
  --config models/configs/labels.yaml \
  --threshold 0.5

# Validation
python src/ner/validate_ner.py \
  --input data/annotated/ \
  --sample 100 \
  --output outputs/reports/ner_quality_metrics.csv
```

**Configuration** : `models/configs/labels.yaml`
```yaml
labels:
  - PERSON
  - ORGANIZATION
  - LOCATION
  - EVENT
  - DATE
```

**Tests** : `tests/test_ner/`

---

### `network/`
**R√¥le** : Construction et analyse de r√©seau d'entit√©s

**Scripts principaux** :
- `build_network.py` - Construction du graphe de co-occurrences
- `analyze_network.py` - Calcul de m√©triques (centralit√©, communaut√©s)
- `export_network.py` - Export GraphML, CSV pour visualisation

**Exemple d'utilisation** :
```bash
# Construction du r√©seau
python src/network/build_network.py \
  --input data/annotated/ \
  --output outputs/networks/entities_network.graphml \
  --min_weight 2

# Analyse
python src/network/analyze_network.py \
  --input outputs/networks/entities_network.graphml \
  --output outputs/reports/network_metrics.csv
```

**Tests** : `tests/test_network/`

---

### `utils/`
**R√¥le** : Fonctions utilitaires r√©utilisables

**Modules** :
- `file_utils.py` - Lecture/√©criture de fichiers (JSON, CSV, TXT)
- `text_utils.py` - Fonctions de traitement de texte (tokenisation, etc.)
- `logging_utils.py` - Configuration de logging standardis√©
- `config.py` - Chargement de configurations (YAML, JSON)

**Exemple** :
```python
from src.utils.file_utils import load_json, save_csv
from src.utils.logging_utils import get_logger

logger = get_logger(__name__)
data = load_json("data/annotated/doc1.json")
logger.info(f"Loaded {len(data['entities'])} entities")
```

---

## üß™ D√©veloppement

### Standards de Code

**Style** : PEP 8 (appliqu√© avec `black` et `flake8`)

**Formatage** :
```bash
# Auto-formatage
black src/

# V√©rification style
flake8 src/
```

**Docstrings** : Format Google (utilisez `@doc_technique`)
```python
def extract_entities(text: str, threshold: float = 0.5) -> List[Dict]:
    """
    Extrait les entit√©s nomm√©es d'un texte avec GLiNER2.

    Args:
        text: Texte √† analyser
        threshold: Seuil de confiance minimum (0.0-1.0)

    Returns:
        Liste de dictionnaires avec cl√©s 'text', 'label', 'start', 'end', 'confidence'

    Raises:
        ValueError: Si threshold n'est pas entre 0 et 1
    """
    ...
```

### Tests Unitaires

**Framework** : pytest

**Lancer les tests** :
```bash
# Tous les tests
pytest

# Tests avec couverture
pytest --cov=src --cov-report=html

# Tests d'un module sp√©cifique
pytest tests/test_ner/
```

**Cr√©er des tests** : Utilisez `@testeur_code`
```bash
@testeur_code Cr√©e des tests pour src/ner/gliner_extractor.py
```

---

## üìö Documentation

**G√©n√©rer documentation** :
```bash
@doc_technique Ajoute docstrings √† tous les fichiers dans src/ner/
```

**README par module** :
Chaque sous-dossier peut avoir son propre README d√©taill√© si n√©cessaire.

---

## üîÑ Pipeline Complet

**Workflow typique** :
```bash
# 1. Preprocessing
python src/preprocessing/clean_text.py --input data/raw/ --output data/processed/

# 2. NER
python src/ner/gliner_extractor.py --input data/processed/ --output data/annotated/

# 3. Validation
python src/ner/validate_ner.py --input data/annotated/ --sample 100

# 4. R√©seau
python src/network/build_network.py --input data/annotated/ --output outputs/networks/
python src/network/analyze_network.py --input outputs/networks/entities_network.graphml
```

**Ou utiliser le pipeline automatique** :
```bash
./scripts/run_full_pipeline.sh
```

---

## ü§ñ Agents Utiles

- `@doc_technique` - Documenter le code
- `@testeur_code` - Cr√©er tests unitaires
- `@nettoyeur_projet` - Refactoring et qualit√©
- `@git_helper` - Messages de commit standardis√©s

---

## üìù TODO

- [ ] D√©velopper `src/preprocessing/clean_text.py`
- [ ] D√©velopper `src/ner/gliner_extractor.py`
- [ ] Cr√©er tests unitaires pour chaque module
- [ ] Documenter toutes les fonctions principales
- [ ] Optimiser performances pour corpus volumineux

---

**Voir aussi** : `docs/METHODOLOGY.md` pour justification des choix techniques
